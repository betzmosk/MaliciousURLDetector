{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "def data_preprocess(path):\r\n",
    "    # Import the dataset\r\n",
    "    data = pd.read_csv(path)\r\n",
    "    # Split the urls by /\r\n",
    "    split = data['URL'].str.split('/', expand=True)\r\n",
    "    # Drop the first column since there was a double slash\r\n",
    "    split.drop([1], axis=1, inplace=True)\r\n",
    "    # get rid of the 'www.'\r\n",
    "    split[2] = split[2].map(lambda x: x.lstrip('www.'))\r\n",
    "    # Create a column with the number of '.' in the url\r\n",
    "    data['num_domain_periods'] = split[2].str.count('\\.')\r\n",
    "    # Create a column with the total length of the url\r\n",
    "    data['domain_length'] = split[2].str.replace('\\.', '', regex=True).str.len()\r\n",
    "    # Create a column with the number of terms in the domain\r\n",
    "    data['num_domain_terms'] = split[2].str.split('\\.').str.len()\r\n",
    "    # Create a blacklist of sensitive words\r\n",
    "    sensitive_words = ['confirm' 'account',\r\n",
    "    'bank', 'secure', 'login', 'signin', 'register', 'update', 'sign-in', 'verify']\r\n",
    "    # Join all of the words in the blacklist with '|'\r\n",
    "    sensitive = '|'.join(sensitive_words)\r\n",
    "    # Create a column of whether a given url contains sensitive words\r\n",
    "    data['Has_Sensitive_words'] = 0\r\n",
    "    data.loc[data.URL.str.contains(sensitive), 'Has_Sensitive_words'] = 1\r\n",
    "    # Create a column of whether a given url contains an IP address\r\n",
    "    data['Has_IP'] = 0\r\n",
    "    data.loc[data.URL.str.contains('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}'), 'Has_IP'] = 1\r\n",
    "    # Create a column that contains the number of periods in the url not including the last three\r\n",
    "    data['Num_Periods'] = data['URL'].str.count('\\.')-3\r\n",
    "    # Create a blacklist of sensitive characters\r\n",
    "    suspicious = ['-', '@', '%']\r\n",
    "    # Join all of the words in the blacklist with '|'\r\n",
    "    suspicious_char = '|'.join(suspicious)\r\n",
    "    # Create a column of whether a given url contains suspicious characters\r\n",
    "    data['Has_sus_char'] = data.URL.str.replace(r':|\\.|/', '', regex=True).str.contains(suspicious_char)\r\n",
    "    data['Has_sus_char'] = data['Has_sus_char'].astype(int)\r\n",
    "    # Create a column for the length of the URL\r\n",
    "    data['URL_Length'] = data.URL.str.len()\r\n",
    "    # Create a column with the number of the slashes in the URL\r\n",
    "    data['num_slashes'] = data.URL.str.count('/')\r\n",
    "    # Create a blacklist for suspicious files\r\n",
    "    files_list = ['.php','.exe','.py','.doc', '.js', '.vb', '.pdf', '.bat', '.dll', '.tmp', '.msi', '.msp', '.ps[12c]', '.lnk', '.inf', 'cmd', 'asp', 'jsp', 'cgi']\r\n",
    "    # Join all of the words in the blacklist with '|'\r\n",
    "    files = '|'.join(files_list)\r\n",
    "    # Create a column of whether a given url contains suspicious_files\r\n",
    "    data['sus_files'] = 0\r\n",
    "    data.loc[data.URL.str.contains(files, case=False), 'sus_files'] = 1\r\n",
    "    # Reorder columns for future column indexing purposes\r\n",
    "    cols_at_end = ['Label']\r\n",
    "    data = data[[c for c in data if c not in cols_at_end] \r\n",
    "            + [c for c in cols_at_end if c in data]]\r\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#splitting training and testing data\r\n",
    "import scipy\r\n",
    "import numpy as np\r\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\r\n",
    "from sklearn.preprocessing import StandardScaler as SScaler\r\n",
    "from sklearn.linear_model import LogisticRegression as LR\r\n",
    "from sklearn.tree import DecisionTreeClassifier as DTClf\r\n",
    "from sklearn.ensemble import RandomForestClassifier as rfClfs\r\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNClf\r\n",
    "from sklearn.metrics import roc_auc_score as auc\r\n",
    "from sklearn.metrics import f1_score as f1\r\n",
    "from sklearn.model_selection import ParameterGrid as PGrid\r\n",
    "from sklearn.svm import SVC as SvmClf\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.neural_network import MLPClassifier as MlpClf\r\n",
    "from xgboost import XGBClassifier as GBClf\r\n",
    "import time\r\n",
    "train = data_preprocess('./Training Data/Phishing_Mitre_Dataset_Summer_of_AI.csv')\r\n",
    "train['Label'] = train['Label'].apply(lambda x: \"+1\" if x == 1 else \"-1\")\r\n",
    "train = train[['Label', 'URL']]\r\n",
    "train.to_csv(r\"C:\\Users\\bmoskowitz\\OneDrive - The MITRE Corporation\\Desktop\\malicious_urls\\urlnet_training.txt\", sep=\"\\t\", header=False, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "add_training = pd.read_csv(r\"C:\\Users\\bmoskowitz\\OneDrive - The MITRE Corporation\\Desktop\\malicious_urls\\runs\\phishing_emb3_dlm0_32dim_minwf1_1conv3456_5ep\\training_output.txt\", delimiter='\\t')['predict']\r\n",
    "train.reset_index(inplace=True)\r\n",
    "train = pd.concat([train, add_training], axis=1)\r\n",
    "train['URLNet_Prediction'] = train['predict'].apply(lambda x: 1 if x == 1 else 0)\r\n",
    "train.drop(columns=['predict'], inplace=True)\r\n",
    "# Define the target column\r\n",
    "y_train = train[\"Label\"]\r\n",
    "y_test = test[\"Label\"]\r\n",
    "x_train = train.drop(columns=[\"Label\", 'URL'])\r\n",
    "x_test = test.drop(columns=[\"Label\", 'URL'])\r\n",
    "x_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#splitting training and testing data\r\n",
    "from sklearn.metrics import roc_auc_score as auc\r\n",
    "from sklearn.metrics import f1_score as f1\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from xgboost import XGBClassifier as GBClf\r\n",
    "\r\n",
    "# Load the training data\r\n",
    "training = data_preprocess('./Training Data/Phishing_Mitre_Dataset_Summer_of_AI.csv')\r\n",
    "# Define the target column\r\n",
    "y_cols = training[\"Label\"]\r\n",
    "# Define the features, exclusing the target and URL \r\n",
    "x_cols = training.drop(columns=[\"Label\", 'URL'])\r\n",
    "# Split the data into a 80% training - 20% test split\r\n",
    "\r\n",
    "gbclf = GBClf(random_state=0, max_depth=6, n_estimators= 100, min_child_weight=1, learning_rate=0.200,\r\n",
    "             use_label_encoder=False, eval_metric='auc').fit(x_cols, y_cols)\r\n",
    "\r\n",
    "# Load the test set\r\n",
    "testing_path = ''\r\n",
    "testing = data_preprocess(path)\r\n",
    "\r\n",
    "# Get the x and y columns\r\n",
    "test_x_col = testing.drop(columns=[\"Label\", 'URL'])\r\n",
    "test_y_col = testing[\"Label\"]\r\n",
    "\r\n",
    "# Predict on the testing set\r\n",
    "labeled_pred_gbclf = gbclf.predict(test_x_col)\r\n",
    "\r\n",
    "# Assess model performance\r\n",
    "print('Gradient Boosting Model')\r\n",
    "print(f'F1 Score: {f1(test_y_col, labeled_pred_gbclf)}')\r\n",
    "print(f'AUC Score: {auc(test_y_col, labeled_pred_gbclf)}')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9c7544dc13e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Load the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtesting_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Get the x and y columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inst_414",
   "language": "python",
   "name": "inst_414"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}